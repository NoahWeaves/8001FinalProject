{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc5ca40",
   "metadata": {},
   "source": [
    "# Check the label_encoder files are all the same between training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d763fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below will:\n",
    "# - Find all label_encoder.joblib files recursively in models\n",
    "# - Load each encoder and compare their classes_ attributes\n",
    "# - Print a clear report showing which encoders match and which differ\n",
    "# - Return True if all are equivalent, False otherwise\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import load\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11cadcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_label_encoders():\n",
    "    # Find all label_encoder.joblib files\n",
    "    encoder_files = []\n",
    "    for root, dirs, files in os.walk(\"models\"):\n",
    "        for file in files:\n",
    "            if file == \"label_encoder.joblib\":\n",
    "                encoder_files.append(os.path.join(root, file))\n",
    "\n",
    "    if not encoder_files:\n",
    "        print(\"No label_encoder.joblib files found.\")\n",
    "        return True\n",
    "\n",
    "    # Load and compare each encoder\n",
    "    encoders = []\n",
    "    for file in encoder_files:\n",
    "        encoder = load(file)\n",
    "        if not isinstance(encoder, LabelEncoder):\n",
    "            print(f\"File {file} is not a LabelEncoder.\")\n",
    "            return False\n",
    "        encoders.append(encoder)\n",
    "\n",
    "    # Compare classes_ attributes\n",
    "    first_classes = encoders[0].classes_\n",
    "    all_equivalent = True\n",
    "    for i, encoder in enumerate(encoders[1:], start=2):\n",
    "        if not np.array_equal(first_classes, encoder.classes_):\n",
    "            print(f\"Encoder {i} differs from the first encoder.\")\n",
    "            all_equivalent = False\n",
    "\n",
    "    return all_equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6bf5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_label_encoders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae773b",
   "metadata": {},
   "source": [
    "# Create comprehensive comparison charts for all baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef0929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, warnings\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# RANDOM_STATE = 42\n",
    "# np.random.seed(RANDOM_STATE)\n",
    "\n",
    "## do not display DtypeWarning from Pandas\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "273f41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_visualizations(summary_df, outdir):\n",
    "    \"\"\"\n",
    "    Create comprehensive comparison charts for all baseline models\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating comparison visualizations...\")\n",
    "    \n",
    "    # 1. Bar chart comparing all metrics across models\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    metrics = ['Accuracy', 'F1_macro', 'Precision_macro', 'Recall_macro']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        summary_df.plot(x='model', y=metric, kind='bar', ax=ax, legend=False, color='steelblue')\n",
    "        ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Model', fontsize=12)\n",
    "        ax.set_ylabel(metric, fontsize=12)\n",
    "        ax.set_xticklabels(summary_df['model'], rotation=45, ha='right')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%.3f', padding=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / \"metrics_comparison_bars.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Radar/Spider chart for multi-metric comparison\n",
    "    from math import pi\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    angles = [n / len(metrics) * 2 * pi for n in range(len(metrics))]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "    \n",
    "    for idx, row in summary_df.iterrows():\n",
    "        values = row[metrics].tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=row['model'])\n",
    "        ax.fill(angles, values, alpha=0.15)\n",
    "    \n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    ax.set_title('Multi-Metric Performance Comparison', size=16, fontweight='bold', pad=20)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / \"metrics_radar_chart.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Heatmap of all metrics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics_matrix = summary_df.set_index('model')[metrics]\n",
    "    sns.heatmap(metrics_matrix, annot=True, fmt='.3f', cmap='YlGnBu', \n",
    "                cbar_kws={'label': 'Score'}, linewidths=0.5)\n",
    "    plt.title('Performance Metrics Heatmap', fontsize=16, fontweight='bold', pad=15)\n",
    "    plt.xlabel('Metrics', fontsize=12)\n",
    "    plt.ylabel('Models', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / \"metrics_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Model ranking visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate average rank for each model\n",
    "    ranks = summary_df[metrics].rank(ascending=False)\n",
    "    ranks['model'] = summary_df['model']\n",
    "    ranks['avg_rank'] = ranks[metrics].mean(axis=1)\n",
    "    ranks = ranks.sort_values('avg_rank')\n",
    "    \n",
    "    x_pos = np.arange(len(ranks))\n",
    "    bars = ax.barh(x_pos, ranks['avg_rank'], color='coral')\n",
    "    ax.set_yticks(x_pos)\n",
    "    ax.set_yticklabels(ranks['model'])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Average Rank (lower is better)', fontsize=12)\n",
    "    ax.set_title('Model Ranking Based on Average Performance', fontsize=16, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, ranks['avg_rank'])):\n",
    "        ax.text(val + 0.05, i, f'{val:.2f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / \"model_ranking.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Box plot showing metric distribution\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    melted = summary_df.melt(id_vars='model', value_vars=metrics, \n",
    "                             var_name='Metric', value_name='Score')\n",
    "    sns.boxplot(data=melted, x='Metric', y='Score', ax=ax, palette='Set2')\n",
    "    sns.swarmplot(data=melted, x='Metric', y='Score', color='black', alpha=0.5, ax=ax)\n",
    "    ax.set_title('Distribution of Metrics Across All Models', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / \"metrics_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"✓ Generated 5 comparison visualizations in {outdir}\")\n",
    "    print(\"  - metrics_comparison_bars.png\")\n",
    "    print(\"  - metrics_radar_chart.png\")\n",
    "    print(\"  - metrics_heatmap.png\")\n",
    "    print(\"  - model_ranking.png\")\n",
    "    print(\"  - metrics_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f565d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_macro</th>\n",
       "      <th>F1_weighted</th>\n",
       "      <th>Precision_macro</th>\n",
       "      <th>Precision_weighted</th>\n",
       "      <th>Recall_macro</th>\n",
       "      <th>Recall_weighted</th>\n",
       "      <th>training_time_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC_GPU</td>\n",
       "      <td>0.983935</td>\n",
       "      <td>0.751124</td>\n",
       "      <td>0.983591</td>\n",
       "      <td>0.824535</td>\n",
       "      <td>0.985091</td>\n",
       "      <td>0.734439</td>\n",
       "      <td>0.983935</td>\n",
       "      <td>3190.860011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost_CUDA</td>\n",
       "      <td>0.989746</td>\n",
       "      <td>0.781287</td>\n",
       "      <td>0.990526</td>\n",
       "      <td>0.764588</td>\n",
       "      <td>0.991875</td>\n",
       "      <td>0.808206</td>\n",
       "      <td>0.989746</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRRegression</td>\n",
       "      <td>0.975162</td>\n",
       "      <td>0.711888</td>\n",
       "      <td>0.974237</td>\n",
       "      <td>0.747405</td>\n",
       "      <td>0.974518</td>\n",
       "      <td>0.707329</td>\n",
       "      <td>0.975162</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN_GPU</td>\n",
       "      <td>0.989518</td>\n",
       "      <td>0.794595</td>\n",
       "      <td>0.989863</td>\n",
       "      <td>0.787207</td>\n",
       "      <td>0.990382</td>\n",
       "      <td>0.802869</td>\n",
       "      <td>0.989518</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForest_GPU</td>\n",
       "      <td>0.986670</td>\n",
       "      <td>0.766923</td>\n",
       "      <td>0.988261</td>\n",
       "      <td>0.749062</td>\n",
       "      <td>0.990827</td>\n",
       "      <td>0.805920</td>\n",
       "      <td>0.986670</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  Accuracy  F1_macro  F1_weighted  Precision_macro  Precision_weighted  Recall_macro  Recall_weighted  training_time_sec\n",
       "0              SVC_GPU  0.983935  0.751124     0.983591         0.824535            0.985091      0.734439         0.983935        3190.860011\n",
       "2         XGBoost_CUDA  0.989746  0.781287     0.990526         0.764588            0.991875      0.808206         0.989746         999.000000\n",
       "3  LogisticRRegression  0.975162  0.711888     0.974237         0.747405            0.974518      0.707329         0.975162         999.000000\n",
       "4              KNN_GPU  0.989518  0.794595     0.989863         0.787207            0.990382      0.802869         0.989518         999.000000\n",
       "5     RandomForest_GPU  0.986670  0.766923     0.988261         0.749062            0.990827      0.805920         0.986670         999.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdir = Path(\"models/baseline_comparisons_no_MLP\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "summary_df = pd.read_csv(\"models/holdout_summary.csv\")\n",
    "summary_df.drop(summary_df[summary_df['model'].str.contains(\"MLP\")].index, inplace=True)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c285d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating comparison visualizations...\n",
      "✓ Generated 5 comparison visualizations in models\\baseline_comparisons_no_MLP\n",
      "  - metrics_comparison_bars.png\n",
      "  - metrics_radar_chart.png\n",
      "  - metrics_heatmap.png\n",
      "  - model_ranking.png\n",
      "  - metrics_distribution.png\n"
     ]
    }
   ],
   "source": [
    "create_comparison_visualizations(summary_df, outdir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
