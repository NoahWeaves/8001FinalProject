Table – Methodological and Reporting Gaps in Sharafaldin et al. (2019)

| **#** | **Aspect**                                      | **What the paper says (or implies)**                                               | **What’s unclear / inconsistent**                                                                                  |
| :---: | :---------------------------------------------- | :--------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------- |
|   1   | **Train/test protocol vs cross-validation**     | Mentions both a *training/testing day split* **and** *five-fold cross-validation*. | Doesn’t explain how CV relates to day split—within-day CV? Across all data (mixing days)? Final test set?          |
|   2   | **Sampling vs full dataset**                    | No statement about sampling; runtime suggests large data volumes.                  | Unclear whether they subsampled for balance/speed or trained on all flows.                                         |
|   3   | **Feature selection scope**                     | Presents per-attack “important features” from a RandomForestRegressor.             | Doesn’t specify if models used **all 80 features**, a **global subset**, or **per-attack subsets**.                |
|   4   | **Use of RandomForestRegressor for importance** | Feature importance computed with a *regressor* even though task is classification. | No justification for using regressor vs `RandomForestClassifier.feature_importances_`.                             |
|   5   | **Preprocessing details**                       | Not discussed.                                                                     | Handling of NaN/∞ values, scaling, normalization, categorical encoding, or class imbalance is unspecified.         |
|   6   | **Hyperparameters / implementations**           | Only hints: “RF 100 trees,” LR took > 2 days, ID3 few minutes.                     | No max_depth, solver, regularization, NB variant, or implementation source given.                                  |
|   7   | **Metric definitions & averaging**              | Reports weighted Precision, Recall, F1.                                            | Doesn’t show per-class results, macro/micro averages, or confusion matrices—hard to gauge minority-class behavior. |
|   8   | **Temporal leakage control**                    | Labels derived from attack-schedule timestamps.                                    | No assurance that CV folds respect time/session boundaries; may mix temporally adjacent flows.                     |
|   9   | **Train/test class mismatch**                   | Training day and testing day have different attack families.                       | Not explained how unseen classes in test are handled (ignored, grouped, or errors).                                |

---

Table - Differences that influence outcomes between our paper and their paper


| Area                             | Paper (Sharafaldin et al., 2019)                                                                                                                                                                                                 | Our code / setup                                                                                                                                                                     | Why it improves results                                                                                                    |
| -------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| **Task framing & metrics**       | Multiclass classification implied; they report weighted Precision/Recall/F1 and say they used five-fold CV, but without fold construction details or per-class results.                                                          | Explicit multiclass; primary **micro-F1/precision/recall** + accuracy; we also log per-class reports and confusion matrices on a **clean holdout**.                                  | Clear, leakage-aware evaluation + richer diagnostics reduces optimistic bias and surfaces minority-class performance.      |
| **Train/validation protocol**    | They describe distinct **training/testing days** and also **five-fold CV**, but don’t explain how CV interacts with the day split (risk of mixing time).                                                                         | CV for model selection (**StratifiedKFold** or **GroupKFold** by timestamp buckets), then **chronological holdout** for final reporting.                                              | Prevents **temporal leakage** and yields more realistic generalization estimates.                                          |
| **Feature set actually used**    | Extract **80 CICFlowMeter features**; perform a custom feature-importance scheme using **RandomForestRegressor** and show per-attack “best features,” but do not say which features the models are trained on (all vs subset).   | Use **SelectKBest(mutual_info_classif)** inside the pipeline (fit only on train folds); `k` tuned during search.                                                                      | Systematic, **leakage-free** selection finds a good subset for the classifier and avoids overfitting to global heuristics. |
| **Preprocessing hygiene**        | No details on handling **NaN/±inf**, scaling, or encoding.                                                                                                                                                                       | Replace `±inf→NaN`; **impute** median (trees: required; XGB: NaN ok); **scale** where needed (LogReg/SVC/kNN/MLP); **LabelEncoder** for targets.                                      | Stable training; many CICFlowMeter rate features produce ±inf—cleaning + correct scaling is often worth several F1 points. |
| **Model family**                 | Four “common ML algorithms”: **ID3**, **Random Forest**, **Naïve Bayes**, **Multinomial Logistic Regression**.                                                                                                                   | Same core families where comparable (**RF**, **LR**), plus modern strong baselines (**SVC**, **kNN**, **MLP**, **XGBoost** if used).                                                  | Even within “same types,” stronger baselines (and better tuning) typically outperform the paper’s out-of-box setups.       |
| **Hyperparameters**              | Only hints: RF “100 trees,” LR training took >2 days; no depth, regularization, solver, etc.                                                                                                                                     | **Bayesian/Randomized search** per model; e.g., LR with **elastic-net + saga**, tuned `C`/`l1_ratio`; RF depth/leafs; SVC `C/kernel`; MLP sizes/lr; XGB full tree/learning-rate grid. | Proper hyperparameter search is one of the largest drivers of lift vs. default settings.                                   |
| **Fold construction / grouping** | Unclear whether CV respected time/session boundaries; labels were created using attack **timestamps**.                                                                                                                           | **Groups by time** (`Timestamp.dt.floor('min')`) or session → **GroupKFold**; no mixing of adjacent flows across folds.                                                               | Reduces optimistic bias; aligns with the capture process (bursty attacks).                                                 |
| **Sampling strategy**            | Not stated; runtimes suggest large sets, but no clarity on **subsampling** or **class balance**.                                                                                                                                 | Transparent: we train on all available rows after cleaning; CV is **stratified** to preserve label proportions; optional `class_weight='balanced'`.                                  | Stratification and optional reweighting mitigate class skew (e.g., UDP-flood dominance) and stabilize metrics.             |
| **Feature-importance method**    | Use **RandomForestRegressor** to score features in a classification task; multiply by per-class standardized means; present per-attack lists. Rationale for regressor vs classifier importance not given.                        | Use a **classifier-appropriate** criterion (mutual information for classification) and select within CV.                                                                              | Matching selector to objective + CV-scoped fitting yields more trustworthy feature sets.                                   |
| **Reporting granularity**        | Provide **weighted averages** (Pr/Rc/F1) and high-level claims (e.g., ID3 best), but no per-class confusion matrices.                                                                                                            | Save **per-class** reports and confusion matrices for both CV and holdout; archive all artifacts.                                                                                     | Makes improvements auditable; reveals which families benefit most (e.g., SYN vs UDP-lag).                                  |
| **Reproducibility**              | No code; many steps (preproc, HPs, folds) unspecified.                                                                                                                                                                           | Fully scripted: deterministic seeds, saved label encoder, configs, CV results and best params.                                                                                        | Easier to replicate/extend; reduces variance due to accidental choices.                                                    |

---

Why Our Baselines Outperform Prior Work

Although we employ similar model families to those in Sharafaldin et al. (2019) [1], our implementation introduces several methodological improvements that directly affect predictive performance and reproducibility.

**1. Explicit and leakage-free evaluation.**
The original paper mentions both a training/testing day split and five-fold cross-validation without clarifying their interaction [1 §IV B, §V B]. We separate model-selection folds (Stratified/Group K-Fold) from a final chronological hold-out, ensuring that temporally adjacent flows never appear in both training and validation sets.

**2. Systematic preprocessing.**
CICFlowMeter features contain infinities and missing values (e.g., flow-rate divisions by zero), which the authors do not address. We explicitly replace ±∞ → NaN, impute medians, scale features where appropriate (for Logistic Regression, SVC, k-NN, MLP), and label-encode the target. These steps stabilize optimization and prevent crashes or implicit data loss.

**3. Feature selection matched to task.**
The paper derives per-attack “important features” using a RandomForestRegressor—a regression model applied to a classification problem [1 §V A, Table IV]. We instead employ SelectKBest(mutual_info_classif) within each CV fold so that feature selection is both classification-aware and fold-local, eliminating information leakage.

**4. Transparent sampling and grouping.**
Sharafaldin et al. do not specify whether they subsampled or balanced classes. We train on the full cleaned dataset with stratified folds and, when needed, class_weight='balanced', which mitigates the strong UDP-flood dominance and yields fairer metrics.

**5. Modern hyperparameter optimization.**
The earlier study fixes hyperparameters (e.g., RF = 100 trees, LR default solver) [1 §V B]. Our pipeline performs Bayesian or randomized searches over depth, regularization, learning rate, and network size, tuning each model family (RF, LR, SVC, k-NN, MLP, XGBoost) independently. This accounts for a substantial portion of the observed F1 improvement.

**6. Appropriate metrics and reporting granularity.**
While the paper reports only weighted Precision, Recall, and F1 averages [1 §V B], we evaluate micro-averaged metrics on both CV and hold-out data and log per-class reports and confusion matrices. This reveals which attack types (e.g., SYN vs UDP-lag) benefit most from each model rather than relying on opaque averages.

**7. Reproducibility.**
All preprocessing, model training, and evaluation steps are scripted with fixed random seeds and logged configurations.
In contrast, many critical implementation details (data sampling, preprocessing, CV folds) are unspecified in the original paper, leading to results that are not directly reproducible.

In combination, these improvements—particularly leakage-aware evaluation, proper preprocessing, and hyperparameter tuning—yield cleaner decision boundaries and systematically higher micro-F1 and accuracy across all baseline classifiers.

**Reference**

[1] M. Sharafaldin, A. Habibi Lashkari, A. A. Ghorbani, Developing Realistic Distributed Denial of Service (DDoS) Attack Dataset and Taxonomy, 2019. Sections IV–V.
